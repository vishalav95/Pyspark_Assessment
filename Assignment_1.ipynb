{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc678d09-7f4b-4dd0-9d09-f7368736e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759fecdd-68c2-4aee-a6f1-a30bc3a55df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions required for Python and Spark SQL.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,to_date, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1931aef0-8ffe-4d0a-b0bc-c99ff4d0b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/08 00:03:36 WARN Utils: Your hostname, Smritis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.29.87 instead (on interface en0)\n",
      "24/02/08 00:03:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/08 00:03:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Assignment_1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd5a449-2f1d-4200-ab8e-d0737a336a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+----+------+----+-----------------+-------------+--------------------+\n",
      "|             gmap_id|             name|pics|rating|resp|             text|         time|             user_id|\n",
      "+--------------------+-----------------+----+------+----+-----------------+-------------+--------------------+\n",
      "|0x7c00159b5b1b1d2...|  manuel grimaldo|NULL|     5|NULL|Great new upgrade|1591839903487|11396541707957662...|\n",
      "|0x7c00159b5b1b1d2...|     Enrique Lara|NULL|     5|NULL|             NULL|1568059018979|11665581913729333...|\n",
      "|0x7c00159b5b1b1d2...|Gregory Donaldson|NULL|     5|NULL|             NULL|1594885588335|10083411999455007...|\n",
      "|0x7c00159b5b1b1d2...|      Brian Baker|NULL|     5|NULL|             NULL|1575951131613|10320721414448209...|\n",
      "|0x7c00159b5b1b1d2...|            Kam J|NULL|     3|NULL|             NULL|1573076723916|10852617116317257...|\n",
      "+--------------------+-----------------+----+------+----+-----------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1504347"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the JSON file from local storage and converting it to a dataframe.\n",
    "# createOrReplaceTempView() function creates a table name that can be used in spark-sql queries.\n",
    "\n",
    "importDF= spark.read.json('/Users/apple/Downloads/review-Hawaii_10.json')\n",
    "importDF.createOrReplaceTempView(\"RatingsAndReviews\")\n",
    "importDF.show(5)\n",
    "importDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220b017-f969-4027-9e1f-d33f5fef3e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad0e42-508c-4f87-9c98-6d52ab7e739a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822c80a-b095-4e64-929f-c6d855ce6387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e20a5c-e2f3-4ef1-8ecf-ab38bc3a7f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+----+------+----+-----------------+-------------+--------------------+\n",
      "|             gmap_id|             name|pics|rating|resp|             text|         time|             user_id|\n",
      "+--------------------+-----------------+----+------+----+-----------------+-------------+--------------------+\n",
      "|0x7c00159b5b1b1d2...|  manuel grimaldo|NULL|     5|NULL|Great new upgrade|1591839903487|11396541707957662...|\n",
      "|0x7c00159b5b1b1d2...|     Enrique Lara|NULL|     5|NULL|             NULL|1568059018979|11665581913729333...|\n",
      "|0x7c00159b5b1b1d2...|Gregory Donaldson|NULL|     5|NULL|             NULL|1594885588335|10083411999455007...|\n",
      "|0x7c00159b5b1b1d2...|      Brian Baker|NULL|     5|NULL|             NULL|1575951131613|10320721414448209...|\n",
      "|0x7c00159b5b1b1d2...|      MONOLITH TV|NULL|     5|NULL|             NULL|1574400673666|11070404066614145...|\n",
      "+--------------------+-----------------+----+------+----+-----------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "887571"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A spark-sql query to fetch all records from the dataframe that have the maximum ratings and show the number of records using the count() function\n",
    "\n",
    "maxRating = spark.sql(\"Select * from RatingsAndReviews where rating = (Select MAX(rating) from RatingsAndReviews)\")\n",
    "maxRating.show(5)\n",
    "maxRating.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe92d0e-ad1a-472c-bcb5-9868778cc072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----+------+----+--------------------+-------------+--------------------+\n",
      "|             gmap_id|           name|pics|rating|resp|                text|         time|             user_id|\n",
      "+--------------------+---------------+----+------+----+--------------------+-------------+--------------------+\n",
      "|0x7c00159b5b1b1d2...|     ethan boam|NULL|     1|NULL|                NULL|1563228631646|10363191684498801...|\n",
      "|0x7c00159b5b1b1d2...|      Josh Snow|NULL|     1|NULL|                NULL|1577824834047|10899063183166499...|\n",
      "|0x7c006de89f2d86e...|Jessica Clopton|NULL|     1|NULL|The doctor is ext...|1545530647643|10578670402504864...|\n",
      "|0x7c006df045b0171...| JILL RODRIGUEZ|NULL|     1|NULL|I went to this pl...|1547454989318|11297488464308832...|\n",
      "|0x7c006df045b0171...|   Michael Owen|NULL|     1|NULL|              Not me|1597966227689|11692291225446130...|\n",
      "+--------------------+---------------+----+------+----+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33115"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A spark-sql query to fetch all records from the dataframe that have the minimum ratings and show the number of records using the count() function\n",
    "\n",
    "minRating = spark.sql(\"Select * from RatingsAndReviews where rating = (Select Min(rating) from RatingsAndReviews)\")\n",
    "minRating.show(5)\n",
    "minRating.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8f448d-5859-4146-a523-07cfb2f2ba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----+------+----+--------------------+-------------+--------------------+\n",
      "|             gmap_id|        name|pics|rating|resp|                text|         time|             user_id|\n",
      "+--------------------+------------+----+------+----+--------------------+-------------+--------------------+\n",
      "|0x7c006d8a56676d2...|shingo inoue|NULL|     5|NULL|(Translated by Go...|1568471997587|11439924047013601...|\n",
      "+--------------------+------------+----+------+----+--------------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#A spark-sql query to fetch the record with the longest review\n",
    "\n",
    "longest_review = spark.sql(\"Select * from RatingsAndReviews where len(text) = (Select MAX(LEN(text)) from RatingsAndReviews)\")\n",
    "longest_review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40b67327-43b6-4604-aad6-8c4bb69a8f52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+----+------+----+-----------------+--------------------+----------+\n",
      "|             gmap_id|             name|pics|rating|resp|             text|             user_id|      date|\n",
      "+--------------------+-----------------+----+------+----+-----------------+--------------------+----------+\n",
      "|0x7c00159b5b1b1d2...|  manuel grimaldo|NULL|     5|NULL|Great new upgrade|11396541707957662...|06-11-2020|\n",
      "|0x7c00159b5b1b1d2...|     Enrique Lara|NULL|     5|NULL|             NULL|11665581913729333...|09-10-2019|\n",
      "|0x7c00159b5b1b1d2...|Gregory Donaldson|NULL|     5|NULL|             NULL|10083411999455007...|07-16-2020|\n",
      "|0x7c00159b5b1b1d2...|      Brian Baker|NULL|     5|NULL|             NULL|10320721414448209...|12-10-2019|\n",
      "|0x7c00159b5b1b1d2...|            Kam J|NULL|     3|NULL|             NULL|10852617116317257...|11-07-2019|\n",
      "+--------------------+-----------------+----+------+----+-----------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1504347"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A spark-sql query to convert the epoch time to date string with mm-dd-yyyy format\n",
    "\n",
    "dateFormattedDF = spark.sql(\"SELECT *, date_format(to_date(from_unixtime(floor(time/1000))), 'MM-dd-yyyy') as date FROM RatingsAndReviews\").drop(\"time\")\n",
    "dateFormattedDF.show(5)\n",
    "dateFormattedDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bfe8717-4275-4e82-afb6-382a3e81d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/08 00:06:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Writing the dataframe to parquet file\n",
    "\n",
    "dateFormattedDF.write.parquet(\"/Users/apple/Downloads/Assignmant_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ba376f4-bc5a-4e49-b1c4-6890b396164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|rating|rating_count|\n",
      "+------+------------+\n",
      "|     1|       33115|\n",
      "|     2|       37119|\n",
      "|     3|      157277|\n",
      "|     4|      389265|\n",
      "|     5|      887571|\n",
      "+------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graphDF = spark.sql(\"Select rating, count(rating) as rating_count from RatingsAndReviews group by rating order by rating\")\n",
    "graphDF.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d395858c-3abc-4256-b75f-311d9976fc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "y_axis = [val.rating for val in graphDF.select('rating').collect()]\n",
    "x_axis = [val.rating_count for val in graphDF.select('rating_count').collect()]\n",
    "\n",
    "plt.plot(x_axis, y_axis_val)\n",
    "\n",
    "plt.ylabel('Ratings')\n",
    "plt.xlabel('Number of ratings')\n",
    "plt.title('Rating vs count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2df5a-10cf-4957-8134-b6864ae2cd99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
